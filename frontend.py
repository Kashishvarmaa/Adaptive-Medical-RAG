import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import json
import time
from PIL import Image
import plotly.express as px
import plotly.graph_objects as go

# Import the Adaptive Med-RAG system
# Comment this out when testing without the actual implementation
# from model import AdaptiveMedRAG

# Mock class for demonstration purposes
class MockAdaptiveMedRAG:
    """Mock class to simulate the AdaptiveMedRAG system"""
    
    def __init__(self, config):
        self.config = config
        self.strategies = ["standard", "soar", "cot", "contrastive", "consistency"]
        
    def index_documents(self):
        """Simulate indexing documents"""
        time.sleep(1)
        return True
    
    def answer_medical_query(self, query, strategy="standard", explain=False):
        """Simulate answering a medical query with a specific strategy"""
        time.sleep(0.5 + np.random.random())  # Add some delay for realism
        
        # Different responses based on strategy
        responses = {
            "standard": f"Standard response to '{query}': This approach uses basic retrieval to find relevant medical information. Metformin is the first-line treatment for type 2 diabetes, combined with lifestyle modifications including diet and exercise.",
            
            "soar": f"SOAR response to '{query}': Using Self-Optimizing Adaptive Retrieval, I found that in patients with type 2 diabetes, metformin remains the cornerstone of therapy due to its efficacy, safety profile, and cost-effectiveness. SGLT2 inhibitors are often added as second-line agents due to their cardiovascular and renal benefits.",
            
            "cot": f"CoT response to '{query}': Let me think step by step about this medical question.\n\n1. First, we need to understand the pathophysiology of type 2 diabetes, which involves insulin resistance and decreased insulin production.\n\n2. First-line treatments target these mechanisms, with metformin reducing hepatic glucose production and improving insulin sensitivity.\n\n3. Lifestyle modifications are equally important, including dietary changes, regular physical activity, and weight management.\n\nTherefore, the recommended first-line approach combines metformin with comprehensive lifestyle modifications.",
            
            "contrastive": f"Contrastive response to '{query}': There are multiple treatment approaches to consider. Metformin offers glucose-lowering benefits without weight gain, while sulfonylureas provide rapid glucose reduction but may cause hypoglycemia. SGLT2 inhibitors offer cardiovascular protection but may increase risk of genital infections. The optimal choice depends on individual patient factors including comorbidities, contraindications, and preferences.",
            
            "consistency": f"Consistency response to '{query}': Analyzing multiple medical guidelines consistently shows that metformin plus lifestyle modification is the universally recommended first-line treatment for type 2 diabetes. This consensus is supported by the American Diabetes Association, European Association for the Study of Diabetes, and the World Health Organization guidelines."
        }
        
        retrieval_info = {
            "docs_retrieved": np.random.randint(3, 8),
            "strategy": strategy,
            "confidence": round(0.7 + np.random.random() * 0.25, 2)
        }
        
        sources = [f"Medical Journal {i}" for i in range(1, np.random.randint(2, 6))]
        
        return {
            "answer": responses.get(strategy, f"Response using {strategy} strategy: This is a simulated answer about {query}"),
            "retrieval": retrieval_info,
            "sources": sources
        }
    
    def personalized_medical_query(self, query, user_id):
        """Simulate personalized medical query"""
        if "researcher" in user_id.lower():
            topics = ["clinical trials", "molecular mechanisms", "research findings"]
            answer = f"Research-oriented response to '{query}': Recent clinical trials have demonstrated significant efficacy of SGLT2 inhibitors in reducing cardiovascular events through multiple molecular pathways including improved cardiac metabolism and reduced cardiac preload."
        else:
            topics = ["treatment protocols", "patient management", "clinical guidelines"]
            answer = f"Clinician-oriented response to '{query}': When prescribing SGLT2 inhibitors, consider patient factors such as eGFR (contraindicated if <30 ml/min/1.73mÂ²), risk of DKA, and need for treatment of concurrent conditions like heart failure or CKD where these agents offer substantial benefits."
        
        return {
            "answer": answer,
            "topics": topics,
            "user_id": user_id
        }

# Setup functions
def create_mock_evaluation_results():
    """Create mock evaluation results for visualization"""
    strategies = ["standard", "soar", "cot", "contrastive", "consistency"]
    metrics = ["accuracy", "f1", "rouge-1", "rouge-l", "bleu"]
    
    # Strategy comparison results
    strategy_results = {}
    for strategy in strategies:
        # Generate slightly better results for 'soar' and 'cot'
        base_score = 0.65
        if strategy == "soar":
            base_score = 0.78
        elif strategy == "cot":
            base_score = 0.75
            
        strategy_results[strategy] = {
            "metrics": {
                metric: round(base_score + np.random.random() * 0.15, 3) for metric in metrics
            },
            "timings": [0.8 + np.random.random() * 2 for _ in range(10)]
        }
    
    # Cross-domain results
    domains = ["general", "pubmed", "emr"]
    domain_results = {}
    for domain in domains:
        domain_results[domain] = {
            "metrics": {
                metric: round(0.6 + np.random.random() * 0.3, 3) for metric in metrics
            }
        }
    
    return {
        "strategy_comparison": {
            "strategies": strategy_results
        },
        "cross_domain": {
            "domains": domain_results
        }
    }

def init_session_state():
    """Initialize session state variables"""
    if 'med_rag' not in st.session_state:
        # Set a configuration for the Med-RAG system
        config = {
            "base_model": "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext",
            "medical_lm": "epfl-llm/meditron-7b",
            "embedding_model": "pritamdeka/S-PubMedBert-MS-MARCO",
            "device": "cpu",  # Change to "cuda" if available
            "data_dir": "./medical_data",
            "index_path": "./vector_indexes",
            "cache_dir": "./retrieval_cache",
            "max_length": 512,
            "chunk_size": 384,
            "chunk_overlap": 128,
            "top_k_default": 5
        }
        
        # Initialize the Med-RAG system
        # Use the mock class for demonstration
        st.session_state.med_rag = MockAdaptiveMedRAG(config)
        # In production, use the actual implementation:
        # st.session_state.med_rag = AdaptiveMedRAG(config)
        
        # Mock evaluation results
        st.session_state.evaluation_results = create_mock_evaluation_results()
        
        # Sample queries
        st.session_state.sample_queries = [
            "What are the first-line treatments for type 2 diabetes?",
            "How does COVID-19 treatment differ between mild and severe cases?",
            "What medications are commonly prescribed for hypertension?",
            "Can you explain the mechanism of action of metformin in diabetes?",
            "What are the treatment targets for hypertension in older adults?",
            "How do SGLT2 inhibitors work in diabetes management?"
        ]

# UI Components
def render_sidebar():
    """Render the sidebar navigation"""
    st.sidebar.image("https://via.placeholder.com/150?text=Med-RAG", width=150)
    st.sidebar.title("Adaptive Med-RAG")
    
    navigation = st.sidebar.radio(
        "Navigation",
        ["Home", "Interactive Demo", "Strategy Comparison", "Evaluation Results", "About"]
    )
    
    st.sidebar.markdown("---")
    st.sidebar.markdown("### System Information")
    st.sidebar.markdown(f"**Base Model:** {st.session_state.med_rag.config['base_model'].split('/')[-1]}")
    st.sidebar.markdown(f"**Medical LM:** {st.session_state.med_rag.config['medical_lm'].split('/')[-1]}")
    
    st.sidebar.markdown("---")
    st.sidebar.markdown("Â© 2025 Adaptive Med-RAG")
    
    return navigation

def render_home():
    """Render the home page"""
    st.title("ðŸ¥ Adaptive Med-RAG")
    st.subheader("Advanced Medical Retrieval-Augmented Generation")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("""
        ### Welcome to Adaptive Med-RAG
        
        Adaptive Med-RAG is a state-of-the-art system for medical question answering that adapts its retrieval strategy based on the query type and context.
        
        **Key Features:**
        - Multiple retrieval strategies (Standard, SOAR, CoT, Contrastive, Consistency)
        - Domain-specific medical knowledge
        - Personalization capabilities
        - Comprehensive evaluation framework
        
        Use the sidebar to navigate through different sections of this application.
        """)
        
        st.markdown("---")
        
        st.markdown("""
        ### Quick Start
        
        Try one of our sample queries or go to the Interactive Demo page to test your own queries:
        """)
        
        sample_query = st.selectbox(
            "Sample Queries",
            st.session_state.sample_queries
        )
        
        sample_strategy = st.selectbox(
            "Strategy",
            st.session_state.med_rag.strategies,
            index=1  # Default to SOAR
        )
        
        if st.button("Run Sample Query"):
            with st.spinner("Processing query..."):
                response = st.session_state.med_rag.answer_medical_query(sample_query, strategy=sample_strategy)
                
                st.markdown("### Response:")
                st.write(response["answer"])
                
                st.markdown("### Retrieval Information:")
                col_a, col_b, col_c = st.columns(3)
                col_a.metric("Documents Retrieved", response["retrieval"]["docs_retrieved"])
                col_b.metric("Strategy", response["retrieval"]["strategy"])
                col_c.metric("Confidence", f"{response['retrieval']['confidence'] * 100:.1f}%")
                
                st.markdown("### Sources:")
                st.write(", ".join(response["sources"]))
    
    with col2:
        st.image("https://via.placeholder.com/400x300?text=Medical+RAG+Diagram", use_column_width=True)
        st.caption("Adaptive Med-RAG System Architecture")
        
        st.markdown("### Strategy Overview")
        
        strategies = {
            "Standard": "Basic retrieval without adaptation",
            "SOAR": "Self-Optimizing Adaptive Retrieval",
            "CoT": "Chain-of-Thought reasoning",
            "Contrastive": "Compare different viewpoints",
            "Consistency": "Ensure consistency across sources"
        }
        
        for strategy, description in strategies.items():
            st.markdown(f"**{strategy}**: {description}")

def render_interactive_demo():
    """Render the interactive demo page"""
    st.title("ðŸ” Interactive Med-RAG Demo")
    st.markdown("Explore the capabilities of Adaptive Med-RAG by asking your own medical questions.")
    
    # User selection
    user_type = st.radio(
        "User Type",
        ["General User", "Medical Researcher", "Clinician"],
        horizontal=True
    )
    
    user_id = f"{user_type.lower().replace(' ', '_')}_user"
    
    # Strategy selection
    col1, col2 = st.columns([3, 2])
    
    with col1:
        query = st.text_area(
            "Enter your medical question",
            value="What are the benefits of SGLT2 inhibitors in diabetic patients with heart failure?",
            height=100
        )
    
    with col2:
        strategy = st.selectbox(
            "Retrieval Strategy",
            st.session_state.med_rag.strategies,
            index=1  # Default to SOAR
        )
        
        explanation = st.checkbox("Include explanation of reasoning", value=True)
    
    # Process query
    if st.button("Submit Query", key="submit_query"):
        st.markdown("---")
        
        with st.spinner("Processing your query..."):
            # Process with personalization if not using "General User"
            if user_type != "General User":
                response = st.session_state.med_rag.personalized_medical_query(query, user_id)
                is_personalized = True
            else:
                response = st.session_state.med_rag.answer_medical_query(query, strategy=strategy, explain=explanation)
                is_personalized = False
            
            # Show processing time
            time.sleep(0.5)  # Simulate additional processing
            
            # Display response
            st.markdown("### Response:")
            st.write(response["answer"])
            
            # Display metadata
            st.markdown("### Query Information:")
            
            if is_personalized:
                col_a, col_b = st.columns(2)
                col_a.metric("User Type", user_type)
                col_b.metric("Personalization", "Enabled")
                
                st.markdown("### User Topics:")
                st.write(", ".join(response["topics"]))
            else:
                col_a, col_b, col_c = st.columns(3)
                col_a.metric("Strategy", strategy)
                col_b.metric("Documents Retrieved", response["retrieval"]["docs_retrieved"])
                col_c.metric("Confidence", f"{response['retrieval']['confidence'] * 100:.1f}%")
                
                st.markdown("### Sources:")
                st.write(", ".join(response["sources"]))
    
    # Strategy comparison
    st.markdown("---")
    st.subheader("Strategy Comparison")
    st.markdown("Compare how different strategies respond to the same query.")
    
    if st.button("Compare All Strategies", key="compare_strategies"):
        all_responses = {}
        
        with st.spinner("Running comparison across all strategies..."):
            for strategy_name in st.session_state.med_rag.strategies:
                all_responses[strategy_name] = st.session_state.med_rag.answer_medical_query(
                    query, strategy=strategy_name, explain=explanation
                )
        
        # Create tabs for each strategy
        tabs = st.tabs(st.session_state.med_rag.strategies)
        
        # Fill each tab
        for i, strategy_name in enumerate(st.session_state.med_rag.strategies):
            with tabs[i]:
                response = all_responses[strategy_name]
                
                st.markdown(f"### {strategy_name.upper()} Strategy Response:")
                st.write(response["answer"])
                
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("**Retrieval Information:**")
                    for key, value in response["retrieval"].items():
                        st.markdown(f"- **{key.replace('_', ' ').title()}**: {value}")
                
                with col2:
                    st.markdown("**Sources:**")
                    for source in response["sources"]:
                        st.markdown(f"- {source}")

def render_strategy_comparison():
    """Render strategy comparison page"""
    st.title("ðŸ“Š Strategy Comparison")
    st.markdown("Compare the performance of different retrieval strategies on medical queries.")
    
    # Create mock comparison data if needed
    strategies = st.session_state.med_rag.strategies
    
    # Performance metrics comparison
    st.subheader("Performance Metrics by Strategy")
    
    # Prepare data for visualization
    metrics_data = []
    for strategy, data in st.session_state.evaluation_results["strategy_comparison"]["strategies"].items():
        for metric, value in data["metrics"].items():
            metrics_data.append({
                "Strategy": strategy.upper(),
                "Metric": metric.upper(),
                "Score": value
            })
    
    metrics_df = pd.DataFrame(metrics_data)
    
    # Create plotly figure
    fig = px.bar(
        metrics_df, 
        x="Metric", 
        y="Score", 
        color="Strategy",
        barmode="group",
        height=500,
        title="Performance Comparison Across Metrics"
    )
    
    fig.update_layout(
        xaxis_title="Metric",
        yaxis_title="Score",
        yaxis_range=[0, 1],
        legend_title="Strategy"
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Response time comparison
    st.subheader("Response Time by Strategy")
    
    # Prepare timing data
    timing_data = {}
    timing_stats = {}
    
    for strategy, data in st.session_state.evaluation_results["strategy_comparison"]["strategies"].items():
        timings = data["timings"]
        timing_data[strategy.upper()] = timings
        timing_stats[strategy.upper()] = {
            "mean": np.mean(timings),
            "median": np.median(timings),
            "min": np.min(timings),
            "max": np.max(timings)
        }
    
    # Create a DataFrame for the box plot
    timing_df = pd.DataFrame({k: pd.Series(v) for k, v in timing_data.items()})
    
    # Create plotly figure
    fig = px.box(
        timing_df,
        points="all",
        height=400,
        title="Response Time Distribution by Strategy"
    )
    
    fig.update_layout(
        xaxis_title="Strategy",
        yaxis_title="Time (seconds)"
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Display timing statistics
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("### Average Response Time")
        for strategy, stats in timing_stats.items():
            st.metric(strategy, f"{stats['mean']:.2f}s")
    
    with col2:
        st.markdown("### Median Response Time")
        for strategy, stats in timing_stats.items():
            st.metric(strategy, f"{stats['median']:.2f}s")
    
    with col3:
        st.markdown("### Response Time Range")
        for strategy, stats in timing_stats.items():
            st.metric(strategy, f"{stats['min']:.2f}s - {stats['max']:.2f}s")
    
    # Strategy details
    st.markdown("---")
    st.subheader("Strategy Details")
    
    strategy_details = {
        "STANDARD": "Basic retrieval strategy that uses vector similarity to find relevant documents.",
        "SOAR": "Self-Optimizing Adaptive Retrieval dynamically adjusts retrieval parameters based on query complexity.",
        "COT": "Chain-of-Thought approach that breaks down complex medical reasoning into logical steps.",
        "CONTRASTIVE": "Presents multiple perspectives by retrieving contrasting information from different sources.",
        "CONSISTENCY": "Prioritizes information that appears consistently across multiple reliable sources."
    }
    
    for strategy, description in strategy_details.items():
        with st.expander(f"{strategy} Strategy"):
            st.write(description)
            
            # Add mock recommendations
            st.markdown("**Best for:**")
            if strategy == "STANDARD":
                st.markdown("- Simple, factual medical questions\n- Quick reference lookups\n- Well-documented standard of care queries")
            elif strategy == "SOAR":
                st.markdown("- Complex diagnostic questions\n- Treatment decision support\n- Questions requiring contextual understanding")
            elif strategy == "COT":
                st.markdown("- Educational explanations\n- Complex reasoning tasks\n- Pathophysiology queries")
            elif strategy == "CONTRASTIVE":
                st.markdown("- Questions with multiple valid approaches\n- Controversial medical topics\n- Treatment comparison queries")
            elif strategy == "CONSISTENCY":
                st.markdown("- Evidence-based medicine questions\n- Clinical guideline queries\n- Safety-critical information")

def render_evaluation_results():
    """Render evaluation results page"""
    st.title("ðŸ“ˆ Evaluation Results")
    st.markdown("Comprehensive evaluation of the Adaptive Med-RAG system across different metrics and domains.")
    
    # Create tabs for different evaluation perspectives
    tab1, tab2, tab3 = st.tabs(["Strategy Performance", "Cross-Domain Evaluation", "Response Quality"])
    
    with tab1:
        st.subheader("Strategy Performance Heatmap")
        
        # Create heatmap data
        heatmap_data = []
        strategies = list(st.session_state.evaluation_results["strategy_comparison"]["strategies"].keys())
        metrics = ["accuracy", "f1", "rouge-1", "rouge-l", "bleu"]
        
        for strategy in strategies:
            strategy_data = st.session_state.evaluation_results["strategy_comparison"]["strategies"][strategy]["metrics"]
            heatmap_data.append([strategy_data.get(m, 0) for m in metrics])
        
        # Create plotly heatmap
        fig = go.Figure(data=go.Heatmap(
            z=heatmap_data,
            x=[m.upper() for m in metrics],
            y=[s.upper() for s in strategies],
            colorscale='Viridis',
            text=[[f"{val:.3f}" for val in row] for row in heatmap_data],
            texttemplate="%{text}",
            textfont={"size":12}
        ))
        
        fig.update_layout(
            title="Strategy Performance by Metric",
            height=500,
            xaxis_title="Metric",
            yaxis_title="Strategy"
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Performance summary
        st.subheader("Overall Strategy Ranking")
        
        # Calculate average score across metrics for each strategy
        avg_scores = {}
        for strategy, data in st.session_state.evaluation_results["strategy_comparison"]["strategies"].items():
            avg_scores[strategy] = np.mean(list(data["metrics"].values()))
        
        # Sort strategies by average score
        sorted_strategies = sorted(avg_scores.items(), key=lambda x: x[1], reverse=True)
        
        # Display ranking
        for i, (strategy, score) in enumerate(sorted_strategies):
            col1, col2 = st.columns([1, 4])
            with col1:
                st.markdown(f"### {i+1}.")
            with col2:
                st.markdown(f"### {strategy.upper()}")
                st.progress(score)
                st.markdown(f"Average Score: **{score:.3f}**")
    
    with tab2:
        st.subheader("Cross-Domain Performance")
        
        # Prepare data for cross-domain visualization
        cross_domain_data = []
        domains = list(st.session_state.evaluation_results["cross_domain"]["domains"].keys())
        metrics = ["accuracy", "f1", "rouge-1", "rouge-l", "bleu"]
        
        for domain in domains:
            domain_metrics = st.session_state.evaluation_results["cross_domain"]["domains"][domain]["metrics"]
            for metric in metrics:
                if metric in domain_metrics:
                    cross_domain_data.append({
                        "Domain": domain.upper(),
                        "Metric": metric.upper(),
                        "Score": domain_metrics[metric]
                    })
        
        # Create DataFrame
        cross_domain_df = pd.DataFrame(cross_domain_data)
        
        # Create plotly chart
        fig = px.bar(
            cross_domain_df,
            x="Domain",
            y="Score",
            color="Metric",
            barmode="group",
            height=500,
            title="Performance Across Medical Domains"
        )
        
        fig.update_layout(
            xaxis_title="Domain",
            yaxis_title="Score",
            yaxis_range=[0, 1],
            legend_title="Metric"
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Domain descriptions
        domain_descriptions = {
            "GENERAL": "General medical knowledge questions spanning multiple topics",
            "PUBMED": "Questions derived from PubMed literature requiring research-level understanding",
            "EMR": "Electronic Medical Record-based questions requiring clinical context"
        }
        
        st.subheader("Domain Descriptions")
        for domain, description in domain_descriptions.items():
            st.markdown(f"**{domain}**: {description}")
    
    with tab3:
        st.subheader("Response Quality Analysis")
        
        # Create mock data for response quality visualization
        quality_metrics = [
            "Factual Accuracy",
            "Completeness",
            "Relevance",
            "Clinical Validity",
            "Citation Quality"
        ]
        
        quality_data = {
            "standard": [0.82, 0.70, 0.85, 0.75, 0.65],
            "soar": [0.88, 0.86, 0.92, 0.90, 0.82],
            "cot": [0.90, 0.85, 0.87, 0.93, 0.80],
            "contrastive": [0.84, 0.88, 0.83, 0.85, 0.75],
            "consistency": [0.86, 0.82, 0.89, 0.88, 0.91]
        }
        
        # Create radar chart with plotly
        fig = go.Figure()
        
        for strategy, values in quality_data.items():
            fig.add_trace(go.Scatterpolar(
                r=values,
                theta=quality_metrics,
                fill='toself',
                name=strategy.upper()
            ))
        
        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 1]
                )
            ),
            title="Response Quality by Strategy",
            showlegend=True,
            height=600
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Expert evaluation results
        st.subheader("Expert Evaluation")
        st.markdown("""
        In addition to automated metrics, responses were evaluated by a panel of medical experts including:
        - 3 Board-certified physicians
        - 2 Medical researchers
        - 1 Clinical pharmacist
        """)
        
        # Mock expert evaluation data
        expert_data = {
            "Clinical Accuracy": [4.2, 4.7, 4.8, 4.5, 4.6],
            "Practical Utility": [3.8, 4.6, 4.3, 4.2, 4.1],
            "Educational Value": [3.9, 4.5, 4.9, 4.4, 4.0]
        }
        
        expert_df = pd.DataFrame(expert_data, index=[s.upper() for s in quality_data.keys()])
        
        st.markdown("**Expert Ratings** (scale: 1-5)")
        st.dataframe(expert_df, use_container_width=True)

def render_about():
    """Render the about page"""
    st.title("â„¹ï¸ About Adaptive Med-RAG")
    
    st.markdown("""
    ## Project Overview
    
    Adaptive Med-RAG is a state-of-the-art medical question answering system that uses Retrieval-Augmented Generation with adaptive strategies. The system dynamically selects the most appropriate retrieval and generation approaches based on the query's complexity, domain, and user context.
    
    ## Key Features
    
    ### Multiple Retrieval Strategies
    - **Standard**: Basic retrieval using vector similarity
    - **SOAR** (Self-Optimizing Adaptive Retrieval): Dynamically adjusts retrieval parameters
    - **CoT** (Chain-of-Thought): Step-by-step reasoning for complex medical questions
    - **Contrastive**: Presents multiple perspectives and approaches
    - **Consistency**: Prioritizes information found consistently across sources
    
    ### Advanced Capabilities
    - Domain-specific medical knowledge index
    - User personalization based on expertise level and preferences
    - Comprehensive evaluation framework
    - Source citation and evidence tracking
    
    ### Technical Details
    - **Base Models**: BiomedNLP-PubMedBERT for embeddings
    - **Language Models**: Specialized medical LLMs like Meditron
    - **Vector Database**: Optimized for medical literature and clinical guidelines
    - **Evaluation Metrics**: ROUGE, BLEU, F1, domain-specific medical accuracy
    """)
    
    st.markdown("---")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ## Use Cases
        
        - **Clinical Decision Support**: Help clinicians find relevant information for patient care
        - **Medical Education**: Assist students in understanding complex medical concepts
        - **Research Assistance**: Support researchers in literature review and hypothesis generation
        - **Patient Information**: Provide reliable medical information in accessible language
        """)
    
    with col2:
        st.markdown("""
        ## System Requirements
        
        - **Python**: 3.8+
        - **Dependencies**: PyTorch, HuggingFace Transformers, pandas, numpy, scikit-learn
        - **Hardware**: CPU system for basic use, GPU recommended for optimal performance
        - **Storage**: 5GB+ for model weights and vector indexes
        """)
    
    st.markdown("---")
    
    st.markdown("""
    ## Usage Instructions
    
    ### Installation
    
    ```bash
    git clone https://github.com/example/adaptive-med-rag.git
    cd adaptive-med-rag
    pip install -r requirements.txt
    ```
    
    ### Basic Usage
    
    ```python
    from adaptive_med_rag import AdaptiveMedRAG
    
    # Initialize the system
    config = {
        "base_model": "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext",
        "medical_lm": "epfl-llm/meditron-7b",
        "embedding_model": "pritamdeka/S-PubMedBert-MS-MARCO"
    }
    
    med_rag = AdaptiveMedRAG(config)
    
    # Answer a medical query
    response = med_rag.answer_medical_query(
        "What are the first-line treatments for type 2 diabetes?",
        strategy="soar"
    )
    
    print(response["answer"])
    ```
    """)
    
    st.markdown("---")
    
    st.markdown("## Licensing and Citation")
